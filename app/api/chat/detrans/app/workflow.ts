import { CachedLLM, RedisCache } from "@/app/api/shared/cache";
import {
  AgentInputData,
  agentStreamEvent,
  createStatefulMiddleware,
  createWorkflow,
  startAgentEvent,
  StatefulContext,
  stopAgentEvent,
  WorkflowContext,
  workflowEvent,
} from "@llamaindex/workflow";
import { type Message } from "ai";
import {
  BaseNode,
  extractText,
  LlamaCloudIndex,
  Memory,
  MessageContent,
  Metadata,
  NodeWithScore,
  VectorStoreIndex,
} from "llamaindex";
import { randomUUID } from "node:crypto";
import { z } from "zod";
import { toSourceEvent } from "../../utils";
import {
  createPlanResearchPrompt,
  researchPrompt,
  writeReportPrompt,
} from "../prompts";
import { getIndex } from "./data";

import { connectRedis } from "@/app/lib/redis";
import { replayCached } from "@/app/lib/replayCached";
import { OpenAI } from "@llamaindex/openai";

const kimi = new OpenAI({
  apiKey: process.env.OPENROUTER_KEY,
  baseURL: "https://openrouter.ai/api/v1",
  model: "moonshotai/kimi-k2",
});
const cache = new RedisCache(await connectRedis(), "detrans");
const llm = new CachedLLM(kimi, cache);

// workflow factory
export const workflowFactory = async (
  chatBody: { messages: Message[] },
  userIp: string,
) => {
  const index = await getIndex(chatBody);
  return getWorkflow(index, userIp);
};

// workflow configs
const MAX_QUESTIONS = 6; // max number of questions to research, research will stop when this number is reached
const TOP_K = 15; // number of nodes to retrieve from the vector store

// workflow events
type ResearchQuestion = { questionId: string; question: string };
type ResearchResult = ResearchQuestion & { answer: string };

// class PlanResearchEvent extends WorkflowEvent<{}> {}
const planResearchEvent = workflowEvent<{}>();
const researchEvent = workflowEvent<ResearchQuestion>();
const reportEvent = workflowEvent<{}>();

type Node = BaseNode<Metadata> & { text: string };

export const UIEventSchema = z
  .object({
    event: z
      .enum(["retrieve", "analyze", "answer"])
      .describe(
        "The type of event. DeepResearch has 3 main stages:\n1. retrieve: Retrieve the context from the vector store\n2. analyze: Analyze the context and generate a research questions to answer\n3. answer: Answer the provided questions. Each question has a unique id, when the state is done, the event will have the answer for the question.",
      ),
    state: z
      .enum(["pending", "inprogress", "done", "error"])
      .describe("The state for each event"),
    id: z.string().optional().describe("The id of the question"),
    question: z
      .string()
      .optional()
      .describe("The question generated by the LLM"),
    answer: z.string().optional().describe("The answer generated by the LLM"),
  })
  .describe("DeepResearchEvent");

type UIEventData = z.infer<typeof UIEventSchema>;

const uiEvent = workflowEvent<{
  type: "ui_event";
  data: UIEventData;
}>();

type DeepResearchState = {
  memory: Memory;
  contextNodes: NodeWithScore<Metadata>[];
  userRequest: MessageContent;
  totalQuestions: number;
  researchResults: Map<string, ResearchResult>;
};
// workflow definition
export function getWorkflow(
  index: VectorStoreIndex | LlamaCloudIndex,
  userIp: string,
) {
  const retriever = index.asRetriever({ similarityTopK: TOP_K });
  const { withState } = createStatefulMiddleware(() => {
    return {
      memory: new Memory([], {}),
      contextNodes: [] as NodeWithScore<Metadata>[],
      userRequest: "" as MessageContent,
      totalQuestions: 0,
      researchResults: new Map<string, ResearchResult>(),
    };
  });
  const workflow = withState(createWorkflow());
  let originalQuestion = "";

  workflow.handle(
    [startAgentEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event: { data: AgentInputData },
    ) => {
      const { userInput, chatHistory } = event.data;
      const { sendEvent, state } = context;

      if (!userInput) throw new Error("Invalid input");
      originalQuestion = userInput as string;

      state.memory.add({ role: "user", content: userInput });
      state.userRequest = userInput;
      sendEvent(
        uiEvent.with({
          type: "ui_event",
          data: {
            event: "retrieve",
            state: "inprogress",
          },
        }),
      );

      let nodes: NodeWithScore<Metadata>[];

      const cachedNodes = await cache.get("nodes:" + userInput);
      if (cachedNodes) {
        nodes = JSON.parse(cachedNodes);
      } else {
        nodes = await retriever.retrieve({ query: userInput });
        await cache.set("nodes:" + userInput, JSON.stringify(nodes));
      }

      // experiment with ranking nodes by reddit score

      const rankedNodes = nodes
        .map((n) => {
          const redditScore = n.node.metadata.score ?? 0; // reddit score
          const normalizedCount = redditScore / 100;
          // explicitly saying: “90% similarity, 10% reddit score.”
          const weightedScore = 0.9 * (n.score || 0) + 0.1 * normalizedCount;
          return {
            ...n,
            score: weightedScore,
          };
        })
        .sort((a, b) => b.score - a.score);

      sendEvent(toSourceEvent(rankedNodes));
      sendEvent(
        uiEvent.with({
          type: "ui_event",
          data: { event: "retrieve", state: "done" },
        }),
      );

      state.contextNodes.push(...rankedNodes);

      return planResearchEvent.with({});
    },
  );

  workflow.handle(
    [planResearchEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event: { data: AgentInputData },
    ) => {
      const { userInput, chatHistory } = event.data;
      const { sendEvent, state, stream } = context;

      sendEvent(
        uiEvent.with({
          type: "ui_event",
          data: { event: "analyze", state: "inprogress" },
        }),
      );

      let decision: any;
      let researchQuestions: any[] = [];
      let cancelReason: string | undefined;

      try {
        const plan = await createResearchPlan(
          state.memory,
          state.contextNodes
            .map(
              (node) =>
                JSON.stringify(node.node.metadata) +
                "comment content: " +
                (node.node as Node).text,
            )
            .join("\n"),
          enhancedPrompt(state.totalQuestions),
          state.userRequest,
          userIp,
        );
        decision = plan.decision;
        researchQuestions = plan.researchQuestions;
        cancelReason = plan.cancelReason;
      } catch (err: any) {
        // rate-limit or other LLM error – send UI event and stop workflow
        sendEvent(
          uiEvent.with({
            type: "ui_event",
            data: {
              event: "analyze",
              state: "error",
              answer:
                "Daily rate limit exceeded for fresh questions. Only cached questions from the front page are available. Please try again tomorrow.",
            },
          }),
        );
        // re-throw so the workflow terminates and the error bubbles up
        throw err;
      }

      sendEvent(
        uiEvent.with({
          type: "ui_event",
          data: { event: "analyze", state: "done" },
        }),
      );
      if (decision === "cancel") {
        sendEvent(
          uiEvent.with({
            type: "ui_event",
            data: { event: "analyze", state: "done" },
          }),
        );
        return agentStreamEvent.with({
          delta: cancelReason ?? "Research cancelled without any reason.",
          response: cancelReason ?? "Research cancelled without any reason.",
          currentAgentName: "",
          raw: null,
        });
      }
      if (decision === "research" && researchQuestions.length > 0) {
        state.totalQuestions += researchQuestions.length;
        state.memory.add({
          role: "assistant",
          content:
            "We need to find answers to the following questions:\n" +
            researchQuestions.join("\n"),
        });
        researchQuestions.forEach(({ questionId: id, question }) => {
          sendEvent(
            uiEvent.with({
              type: "ui_event",
              data: { event: "answer", state: "pending", id, question },
            }),
          );
          sendEvent(researchEvent.with({ questionId: id, question }));
        });
        const events = await stream
          .until(() => state.researchResults.size === researchQuestions.length)
          .toArray();
        return planResearchEvent.with({});
      } else {
        state.memory.add({
          role: "assistant",
          content: "No more idea to analyze. We should report the answers.",
        });
        sendEvent(
          uiEvent.with({
            type: "ui_event",
            data: { event: "analyze", state: "done" },
          }),
        );
        return reportEvent.with({});
      }
    },
  );

  workflow.handle(
    [researchEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event,
    ) => {
      const { sendEvent, state } = context;
      const { questionId, question } = event.data;

      sendEvent(
        uiEvent.with({
          type: "ui_event",
          data: {
            event: "answer",
            state: "inprogress",
            id: questionId,
            question,
          },
        }),
      );

      try {
        const answer = await answerQuestion(
          contextStr(state.contextNodes),
          question,
        );
        state.researchResults.set(questionId, { questionId, question, answer });

        state.memory.add({
          role: "assistant",
          content: `<Question>${question}</Question>\n<Answer>${answer}</Answer>`,
        });

        sendEvent(
          uiEvent.with({
            type: "ui_event",
            data: {
              event: "answer",
              state: "done",
              id: questionId,
              question,
              answer,
            },
          }),
        );
      } catch (err: any) {
        // forward the error to the UI and then re-throw so the workflow fails
        sendEvent(
          uiEvent.with({
            type: "ui_event",
            data: {
              event: "answer",
              state: "error",
              id: questionId,
              question,
              answer: err.message || "LLM error",
            },
          }),
        );
        throw err;
      }
    },
  );

  workflow.handle(
    [reportEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event,
    ) => {
      const { sendEvent, state } = context;
      const chatHistory = await state.memory.get();
      const messages = chatHistory.concat([
        {
          role: "system",
          content: writeReportPrompt,
        },
        {
          role: "user",
          content:
            "Write a summary addressing the user request based on the research provided in the context",
        },
      ]);

      let response = "";
      let stream;

      const cachedAnswer = await cache.get("answer:" + originalQuestion);

      if (cachedAnswer) {
        stream = replayCached(cachedAnswer);
      } else {
        stream = await llm.chat({ messages, stream: true });
      }

      for await (const chunk of stream) {
        response += chunk.delta;
        sendEvent(
          agentStreamEvent.with({
            delta: chunk.delta,
            response,
            currentAgentName: "",
            raw: stream,
          }),
        );
      }

      if (!cachedAnswer) {
        await cache.set("answer:" + originalQuestion, response);
      }

      return stopAgentEvent.with({
        result: response,
        message: { role: "assistant", content: "the reseach is complete" },
      });
    },
  );

  return workflow;
}

const createResearchPlan = async (
  memory: Memory,
  contextStr: string,
  enhancedPrompt: string,
  userRequest: MessageContent,
  userIp: string,
) => {
  const chatHistory = await memory.get();

  const conversationContext = chatHistory
    .map((message) => `${message.role}: ${message.content}`)
    .join("\n");

  const prompt = createPlanResearchPrompt(MAX_QUESTIONS).format({
    context_str: contextStr,
    conversation_context: conversationContext,
    enhanced_prompt: enhancedPrompt,
    user_request: extractText(userRequest),
  });

  const responseFormat = z.object({
    decision: z.enum(["research", "write", "cancel"]),
    researchQuestions: z.array(z.string()),
    cancelReason: z.string(),
  });

  const result = await llm.complete({
    prompt,
    responseFormat,
    rateLimit: { userIp, mode: "detrans" },
  });
  const plan = JSON.parse(result.text) as z.infer<typeof responseFormat>;

  return {
    ...plan,
    researchQuestions: plan.researchQuestions.map((question) => ({
      questionId: randomUUID(),
      question,
    })),
  };
};

const contextStr = (contextNodes: NodeWithScore<Metadata>[]) => {
  return contextNodes
    .map((n) => {
      const node = n.node as Node;
      const nodeId = node.id_;
      const meta = node.metadata;
      const nodeContent = node.text;
      return `<Citation id='${nodeId}' redditUser='${meta.username}' link='${meta.link}' type='${meta.type}' >\n${nodeContent}</Citation id='${nodeId}'>`;
    })
    .join("\n");
};

const enhancedPrompt = (totalQuestions: number) => {
  if (totalQuestions === 0) {
    return "The student has no questions to research. Let start by providing some questions for the student to research.";
  }

  if (totalQuestions >= MAX_QUESTIONS) {
    return `The student has researched ${totalQuestions} questions. Should proceeding writing summary or cancel the research if the answers are not enough to write a summary.`;
  }

  return "";
};

const answerQuestion = async (contextStr: string, question: string) => {
  const prompt = researchPrompt.format({
    context_str: contextStr,
    question,
  });
  const result = await llm.complete({ prompt });
  return result.text;
};
