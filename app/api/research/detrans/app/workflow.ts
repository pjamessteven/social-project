import { CachedLLM, PostgresCache } from "@/app/api/shared/cache";
import { getCachedAnswer, setCachedAnswer } from "@/app/lib/cache";
import { replayCached } from "@/app/lib/replayCached";
import { getLogger } from "@/app/lib/logger";
import { OpenAI } from "@llamaindex/openai";
import {
  AgentInputData,
  agentStreamEvent,
  createStatefulMiddleware,
  createWorkflow,
  startAgentEvent,
  StatefulContext,
  stopAgentEvent,
  WorkflowContext,
  workflowEvent,
} from "@llamaindex/workflow";
import { type UIMessage } from "ai";
import {
  BaseNode,
  extractText,
  Memory,
  MessageContent,
  Metadata,
  NodeWithScore,
  VectorStoreIndex,
} from "llamaindex";
import { randomUUID } from "node:crypto";
import { z } from 'zod/v3';
import { toSourceEvent } from "../../utils";
import {
  createPlanResearchPrompt,
  researchPrompt,
  writeReportPrompt,
} from "../prompts";
import { getIndex } from "./data";

const kimi = new OpenAI({
  apiKey: process.env.OPENROUTER_KEY,
  baseURL: "https://openrouter.ai/api/v1",
  model: "moonshotai/kimi-k2",
});

const cache = new PostgresCache("detrans");
const llm = new CachedLLM(kimi, cache, "detrans");

// workflow factory
export const workflowFactory = async (
  chatBody: { messages: UIMessage[] },
  userIp: string,
) => {
  const index = await getIndex(chatBody);
  return getWorkflow(index, userIp);
};

// workflow configs
const MAX_QUESTIONS = 6; // max number of questions to research, research will stop when this number is reached
const TOP_K = 15; // number of nodes to retrieve from the vector store

// workflow events
type ResearchQuestion = { questionId: string; question: string };
type ResearchResult = ResearchQuestion & { answer: string };

// class PlanResearchEvent extends WorkflowEvent<{}> {}
const planResearchEvent = workflowEvent<{}>();
const researchEvent = workflowEvent<ResearchQuestion>();
const reportEvent = workflowEvent<{}>();

type Node = BaseNode<Metadata> & { text: string };

export const UIEventSchema = z
  .object({
    event: z
      .enum(["retrieve", "analyze", "answer"])
      .describe(
        "The type of event. DeepResearch has 3 main stages:\n1. retrieve: Retrieve the context from the vector store\n2. analyze: Analyze the context and generate a research questions to answer\n3. answer: Answer the provided questions. Each question has a unique id, when the state is done, the event will have the answer for the question.",
      ),
    state: z
      .enum(["pending", "inprogress", "done", "error"])
      .describe("The state for each event"),
    id: z.string().optional().describe("The id of the question"),
    question: z
      .string()
      .optional()
      .describe("The question generated by the LLM"),
    answer: z.string().optional().describe("The answer generated by the LLM"),
  })
  .describe("DeepResearchEvent");

type UIEventData = z.infer<typeof UIEventSchema>;

const uiEvent = workflowEvent<{
  type: "data-ui_event";
  data: UIEventData;
}>();

type DeepResearchState = {
  memory: Memory;
  contextNodes: NodeWithScore<Metadata>[];
  userRequest: MessageContent;
  totalQuestions: number;
  researchResults: Map<string, ResearchResult>;
  isReporting?: boolean;
};
// workflow definition
export function getWorkflow(index: VectorStoreIndex, userIp: string) {
  const retriever = index.asRetriever({ similarityTopK: TOP_K });
  const { withState } = createStatefulMiddleware(() => {
    return {
      memory: new Memory([], {}),
      contextNodes: [] as NodeWithScore<Metadata>[],
      userRequest: "" as MessageContent,
      totalQuestions: 0,
      researchResults: new Map<string, ResearchResult>(),
      isReporting: false,
    };
  });
  const workflow = withState(createWorkflow());
  let originalQuestion = "";

  workflow.handle(
    [startAgentEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event: { data: AgentInputData },
    ) => {
      const { userInput, chatHistory } = event.data;
      const { sendEvent, state } = context;

      if (!userInput) throw new Error("Invalid input");
      originalQuestion = userInput as string;

      state.memory.add({ role: "user", content: userInput });
      state.userRequest = userInput;
      sendEvent(
        uiEvent.with({
          type: "data-ui_event",
          data: {
            event: "retrieve",
            state: "inprogress",
          },
        }),
      );

      let nodes: NodeWithScore<Metadata>[];
      const logger = getLogger();
      const nodesKey = originalQuestion + ":nodes";

      getLogger().info({
        originalQuestion,
        nodesKey,
        keyLength: nodesKey.length,
        mode: 'detrans',
        type: 'cache_debug'
      }, 'Looking for cached nodes with key');

      const nodesStartTime = Date.now();
      const cachedNodes = await cache.get(nodesKey);
      if (cachedNodes) {
        const parseStartTime = Date.now();
        nodes = JSON.parse(cachedNodes);
        const parseTime = Date.now() - parseStartTime;
        const totalTime = Date.now() - nodesStartTime;
        logger.info({
          originalQuestion,
          cacheKey: 'nodes',
          mode: 'detrans',
          type: 'workflow_cache',
          cacheRetrievalTime: nodesStartTime,
          parseTime,
          totalTime
        }, 'Workflow cache hit (nodes)');
      } else {
        const retrievalStartTime = Date.now();
        nodes = await retriever.retrieve({ query: originalQuestion });
        const retrievalTime = Date.now() - retrievalStartTime;
        
        const cacheSetStartTime = Date.now();
        await cache.set(
          nodesKey,
          JSON.stringify(nodes),
          originalQuestion,
        );
        const cacheSetTime = Date.now() - cacheSetStartTime;
        const totalTime = Date.now() - nodesStartTime;
        
        logger.info({
          originalQuestion,
          cacheKey: 'nodes',
          mode: 'detrans',
          type: 'workflow_cache',
          retrievalTime,
          cacheSetTime,
          totalTime
        }, 'Workflow cache miss, retrieving nodes');
      }

      // experiment with ranking nodes by reddit score

      const rankedNodes = nodes
        .map((n) => {
          const redditScore = n.node.metadata.score ?? 0; // reddit score
          const normalizedCount = redditScore / 100;
          // explicitly saying: “90% similarity, 10% reddit score.”
          const weightedScore = 0.9 * (n.score || 0) + 0.1 * normalizedCount;
          return {
            ...n,
            score: weightedScore,
          };
        })
        .sort((a, b) => b.score - a.score);

      sendEvent(toSourceEvent(rankedNodes));
      sendEvent(
        uiEvent.with({
          type: "data-ui_event",
          data: { event: "retrieve", state: "done" },
        }),
      );

      state.contextNodes.push(...rankedNodes);

      return planResearchEvent.with({});
    },
  );

  workflow.handle(
    [planResearchEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event: { data: AgentInputData },
    ) => {
      const { userInput, chatHistory } = event.data;
      const { sendEvent, state, stream } = context;

      sendEvent(
        uiEvent.with({
          type: "data-ui_event",
          data: { event: "analyze", state: "inprogress" },
        }),
      );

      let decision: any;
      let researchQuestions: any[] = [];
      let cancelReason: string | undefined;

      try {
        const planStartTime = Date.now();
        const plan = await createResearchPlan(
          state.memory,
          state.contextNodes
            .map(
              (node) =>
                JSON.stringify(node.node.metadata) +
                "comment content: " +
                (node.node as Node).text,
            )
            .join("\n"),
          enhancedPrompt(state.totalQuestions),
          state.userRequest,
          userIp,
        );
        const planTime = Date.now() - planStartTime;
        
        getLogger().info({
          originalQuestion,
          mode: 'detrans',
          type: 'workflow_timing',
          step: 'create_research_plan',
          planTime
        }, 'Research plan creation completed');
        
        decision = plan.decision;
        researchQuestions = plan.researchQuestions;
        cancelReason = plan.cancelReason;
      } catch (err: any) {
        // rate-limit or other LLM error – send UI event and stop workflow
        sendEvent(
          uiEvent.with({
            type: "data-ui_event",
            data: {
              event: "analyze",
              state: "error",
              answer:
                "Daily rate limit exceeded for fresh questions. Only cached questions from the front page are available. Please try again tomorrow.",
            },
          }),
        );
        // re-throw so the workflow terminates and the error bubbles up
        throw err;
      }

      sendEvent(
        uiEvent.with({
          type: "data-ui_event",
          data: { event: "analyze", state: "done" },
        }),
      );
      if (decision === "cancel") {
        sendEvent(
          uiEvent.with({
            type: "data-ui_event",
            data: { event: "analyze", state: "done" },
          }),
        );
        return agentStreamEvent.with({
          delta: cancelReason ?? "Research cancelled without any reason.",
          response: cancelReason ?? "Research cancelled without any reason.",
          currentAgentName: "",
          raw: null,
        });
      }
      if (decision === "research" && researchQuestions.length > 0) {
        state.totalQuestions += researchQuestions.length;
        state.memory.add({
          role: "assistant",
          content:
            "We need to find answers to the following questions:\n" +
            researchQuestions.join("\n"),
        });
        researchQuestions.forEach(({ questionId: id, question }) => {
          sendEvent(
            uiEvent.with({
              type: "data-ui_event",
              data: { event: "answer", state: "pending", id, question },
            }),
          );
          sendEvent(researchEvent.with({ questionId: id, question }));
        });
        const events = await stream
          .until(() => state.researchResults.size === researchQuestions.length)
          .toArray();
        
        // Sort results by questionId to ensure deterministic order
        const sortedResults = Array.from(state.researchResults.values())
          .sort((a, b) => a.questionId.localeCompare(b.questionId));

        // Clear memory of research results and add them back in sorted order
        const baseMemoryMessages = await state.memory.get();
        // Find the index where research results start (after "We need to find answers...")
        const researchStartIndex = baseMemoryMessages.findIndex(msg => 
          msg.role === "assistant" && 
          String(msg.content).includes("We need to find answers to the following questions:")
        );
        
        if (researchStartIndex !== -1) {
          // Keep only messages before research results
          const baseMessages = baseMemoryMessages.slice(0, researchStartIndex + 1);
          state.memory = new Memory(baseMessages, {});
          
          // Add sorted research results
          sortedResults.forEach(({ question, answer }) => {
            state.memory.add({
              role: "assistant",
              content: `<Question>${question}</Question>\n<Answer>${answer}</Answer>`,
            });
          });
          
          getLogger().info({
            originalQuestion,
            mode: 'detrans',
            type: 'memory_debug',
            sortedQuestionIds: sortedResults.map(r => r.questionId),
            researchStartIndex
          }, 'Reordered memory with sorted research results');
        }
        
        return planResearchEvent.with({});
      }
      state.memory.add({
        role: "assistant",
        content: "No more idea to analyze. We should report the answers.",
      });
      sendEvent(
        uiEvent.with({
          type: "data-ui_event",
          data: { event: "analyze", state: "done" },
        }),
      );
      return reportEvent.with({});
    },
  );

  workflow.handle(
    [researchEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event,
    ) => {
      const { sendEvent, state } = context;
      const { questionId, question } = event.data;

      sendEvent(
        uiEvent.with({
          type: "data-ui_event",
          data: {
            event: "answer",
            state: "inprogress",
            id: questionId,
            question,
          },
        }),
      );

      try {
        const answerStartTime = Date.now();
        const answer = await answerQuestion(
          contextStr(state.contextNodes),
          question,
          originalQuestion,
        );
        const answerTime = Date.now() - answerStartTime;
        
        getLogger().info({
          originalQuestion,
          questionId,
          mode: 'detrans',
          type: 'workflow_timing',
          step: 'answer_question',
          answerTime
        }, 'Question answered');
        
        state.researchResults.set(questionId, { questionId, question, answer });

        // Don't add to memory here - we'll add all results in sorted order later
        getLogger().info({
          originalQuestion,
          questionId,
          mode: 'detrans',
          type: 'research_debug'
        }, 'Research result stored, will be added to memory in sorted order');

        sendEvent(
          uiEvent.with({
            type: "data-ui_event",
            data: {
              event: "answer",
              state: "done",
              id: questionId,
              question,
              answer,
            },
          }),
        );
      } catch (err: any) {
        // forward the error to the UI and then re-throw so the workflow fails
        sendEvent(
          uiEvent.with({
            type: "data-ui_event",
            data: {
              event: "answer",
              state: "error",
              id: questionId,
              question,
              answer: err.message || "LLM error",
            },
          }),
        );
        throw err;
      }
    },
  );

  workflow.handle(
    [reportEvent],
    async (
      context: StatefulContext<DeepResearchState, WorkflowContext>,
      event,
    ) => {
      const { sendEvent, state } = context;
      const chatHistory = await state.memory.get();
      state.isReporting = true;
      
      getLogger().info({
        originalQuestion,
        mode: 'detrans',
        type: 'memory_debug',
        memoryLength: chatHistory.length,
        memoryContent: chatHistory.map((msg, i) => `${i}: ${msg.role}: ${String(msg.content).substring(0, 100)}`).join('\n')
      }, 'Memory content before final report');
      
      const messages = chatHistory.concat([
        {
          role: "system",
          content: writeReportPrompt,
        },
        {
          role: "user",
          content:
            "Write a summary addressing the user request. Use ONLY the research provided in the context",
        },
      ]);

      let response = "";
      let stream;

      getLogger().info({
        originalQuestion,
        questionLength: originalQuestion.length,
        mode: 'detrans',
        type: 'cache_debug'
      }, 'Looking for cached final answer');

      const finalAnswerStartTime = Date.now();
      const cachedAnswer = await getCachedAnswer("detrans", originalQuestion);
      const cacheCheckTime = Date.now() - finalAnswerStartTime;

      if (cachedAnswer) {
        const replayStartTime = Date.now();
        stream = replayCached(cachedAnswer);
        const replaySetupTime = Date.now() - replayStartTime;
        
        getLogger().info({
          originalQuestion,
          cacheKey: 'final_answer',
          mode: 'detrans',
          type: 'workflow_cache',
          cacheCheckTime,
          replaySetupTime
        }, 'Workflow cache hit (final answer)');
      } else {
        const llmStartTime = Date.now();
        stream = await llm.chat({ messages, originalQuestion, stream: true });
        const llmSetupTime = Date.now() - llmStartTime;
        
        getLogger().info({
          originalQuestion,
          cacheKey: 'final_answer',
          mode: 'detrans',
          type: 'workflow_cache',
          cacheCheckTime,
          llmSetupTime
        }, 'Workflow cache miss, generating final answer');
      }

      for await (const chunk of stream) {
        response += chunk.delta;
        sendEvent(
          agentStreamEvent.with({
            delta: chunk.delta,
            response,
            currentAgentName: "LLM",
            raw: chunk.raw,
          }),
        );
      }

      if (!cachedAnswer) {
        await setCachedAnswer("detrans", originalQuestion, response);
      }

      return stopAgentEvent.with({
        result: response,
        message: { role: "assistant", content: "the reseach is complete" },
      });
    },
  );

  return workflow;
}

const createResearchPlan = async (
  memory: Memory,
  contextStr: string,
  enhancedPrompt: string,
  userRequest: MessageContent,
  userIp: string,
) => {
  const chatHistory = await memory.get();

  const conversationContext = chatHistory
    .map((message) => `${message.role}: ${message.content}`)
    .join("\n");

  const prompt = createPlanResearchPrompt(MAX_QUESTIONS).format({
    context_str: contextStr,
    conversation_context: conversationContext,
    enhanced_prompt: enhancedPrompt,
    user_request: extractText(userRequest),
  });

  const responseFormat = z.object({
    decision: z.enum(["research", "write", "cancel"]),
    researchQuestions: z.array(z.string()),
    cancelReason: z.string(),
  });

  const result = await llm.complete({
    originalQuestion: extractText(userRequest),
    prompt,
    responseFormat,
    rateLimit: { userIp, mode: "detrans" },
  });
  const plan = JSON.parse(result.text) as z.infer<typeof responseFormat>;

  return {
    ...plan,
    researchQuestions: plan.researchQuestions.map((question) => ({
      questionId: randomUUID(),
      question,
    })),
  };
};

const contextStr = (contextNodes: NodeWithScore<Metadata>[]) => {
  return contextNodes
    .map((n) => {
      const node = n.node as Node;
      const nodeId = node.id_;
      const meta = node.metadata;
      const nodeContent = node.text;
      return `<Citation id='${nodeId}' redditUser='${meta.username}' link='${meta.link}' type='${meta.type}' >\n${nodeContent}</Citation id='${nodeId}'>`;
    })
    .join("\n");
};

const enhancedPrompt = (totalQuestions: number) => {
  if (totalQuestions === 0) {
    return "The student has no questions to research. Let start by providing some questions for the student to research.";
  }

  if (totalQuestions >= MAX_QUESTIONS) {
    return `The student has researched ${totalQuestions} questions. Should proceeding writing summary or cancel the research if the answers are not enough to write a summary.`;
  }

  return "";
};

const answerQuestion = async (
  contextStr: string,
  question: string,
  originalQuestion: string,
) => {
  const prompt = researchPrompt.format({
    context_str: contextStr,
    question,
  });
  const result = await llm.complete({ prompt, originalQuestion });
  return result.text;
};
